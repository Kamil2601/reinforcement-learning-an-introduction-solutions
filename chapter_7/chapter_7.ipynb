{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.1\n",
    "\n",
    "Standard update rule for $n$-step bootstrapping:\n",
    "\n",
    "$\n",
    "G_{t:t+n} = R_{t+1} + \\gamma R_{t+2} + ... + \\gamma^{n-1}R_{t+n} + \\gamma^n V_{t+n-1}(S_{t+n}) = R_{t+1} + \\gamma G_{t+1:t+n}\n",
    "$\n",
    "\n",
    "$\n",
    "V_{t+n}(S_t) = V_{t+n-1}(S_t) + \\alpha \\left[G_{t:t+n} - V_{t+n-1}(S_t) \\right]\n",
    "$\n",
    "\n",
    "If the estimation doesn't change after from step to step, then $n$-step return is equal to:\n",
    "\n",
    "$\n",
    "G_{t:t+n} = R_{t+1} + \\gamma R_{t+2} + ... + \\gamma^{n-1}R_{t+n} + \\gamma^n V(S_{t+n}) = R_{t+1} + \\gamma G_{t+1:t+n}\n",
    "$\n",
    "\n",
    "and the $n$-step error is equal to\n",
    "$\n",
    "G_{t:t+n} - V(S_t)\n",
    "$.\n",
    "\n",
    "TD-error:\n",
    "\n",
    "$\n",
    "\\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)\n",
    "$\n",
    "\n",
    "So\n",
    "\n",
    "\\begin{split}\n",
    "G_t - V(S_t) &= R_{t+1} + \\gamma G_{t+1:t+n} - V(S_t) \\\\\n",
    "             &= R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) - \\gamma V(S_{t+1}) + \\gamma G_{t+1:t+n} \\\\\n",
    "             &= \\delta_t + \\gamma (G_{t+1:t+n} - V(S_{t+1})) \\\\\n",
    "             &= \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2 (G_{t+2:t+n} - V(S_{t+2})) \\\\\n",
    "             &= \\delta_t + \\gamma \\delta_{t+1} + ... + \\gamma^{n-1} \\delta_{t+n-1} + \\gamma^n(G_{t+n:t+n} - V(S_{t+n})) \\\\\n",
    "             &= \\sum_{k=0}^{n-1} \\gamma^k \\delta_{t+k} + \\gamma^n(V(S_{t+n}) - V(S_{t+n})) \\\\\n",
    "             &= \\sum_{k=0}^{n-1} \\gamma^k \\delta_{t+k}\n",
    "\\end{split}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.3\n",
    "Larger random walk task was used, so the episodes are longer. For smaller tasks the cases of larger $n$ (32 and more), would work like Monte Carlo for most of the episodes, so presumably there wouldn't be a clear difference between e.g. $n$ of 64 and 128. $n$-step bootstrapping works better for longer episodes, so smaller task would probably shift the advantage towards smaller $n$. Change in left-side outcome from 0 to -1 makes no difference if the initial estimation is also changed from 0.5 to 0. The only differnce would be the larger gap between the estimation (and the true value) of each two states. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
