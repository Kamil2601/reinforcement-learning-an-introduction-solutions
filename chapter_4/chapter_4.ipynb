{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 4.1\n",
    "$$\n",
    "q_\\pi(11, \\text{down}) = -1 + v_\\pi(s_{terminal}) = -1 + 0 = -1\n",
    "$$\n",
    "\n",
    "$$\n",
    "q_\\pi(7, \\text{down}) = -1 + v_\\pi(11) = -1 + (-14) = -15\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 4.2\n",
    "\n",
    "a) If the transitions of the original states are unchanged\n",
    "$$\n",
    "v_\\pi(15) = -1 + 0.25 * (-22) + 0.25 * (-20) + 0.25 * (-14) + 0.25 * v_\\pi(15)\n",
    "$$\n",
    "$$\n",
    "4v_\\pi(15) = -4 -22 - 20 - 14 + v_\\pi(15)\n",
    "$$\n",
    "$$\n",
    "3v_\\pi(15) = -60\n",
    "$$\n",
    "$$\n",
    "v_\\pi(15) = -20\n",
    "$$\n",
    "\n",
    "b) If the transitions changed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 4.3\n",
    "$$\n",
    "q_{k+1}(s, a) = \\sum_{s', r} p(s', r | s, a) \\left[r + \\gamma \\sum_{a'} \\pi(a'|s') q_k(s', a') \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 4.4\n",
    "\n",
    "The problem is that the $\\argmax_a$ breaks the ties arbitrarily, so it can switch between actions of equal value forever.\n",
    "\n",
    "It can be solved by assigning a unique number $n(a)$ to each action $a$, and replace $\\argmax_a q_\\pi(s, a)$ with $\\argmax_a (q_\\pi(s, a), n(a))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 4.5\n",
    "\n",
    "#### 1. Initialization\n",
    "$Q(s, a) \\in \\mathbb{R} $ and $\\pi(s) \\in \\mathcal{A}$ arbitrarily for all $s \\in \\mathcal{S}$ and $a \\in \\mathcal{A}$; $Q(terminal, a) = 0$\n",
    "\n",
    "#### 2. Policy Evaluation\n",
    "1. Loop:\n",
    "    1. $\\Delta \\leftarrow 0$\n",
    "    2. Loop for each $s \\in S$, $a \\in \\mathcal{A}$\n",
    "        1. $q \\leftarrow Q(s, a)$\n",
    "        2. $Q(s, a) = \\sum_{s', r} p(s', r | s, a) \\left[r + \\gamma Q(s', \\pi(s')) \\right]$\n",
    "        3. $\\Delta \\leftarrow max(\\Delta, |q - Q(s,a)|)$\n",
    "2. until $\\Delta < \\theta$\n",
    "\n",
    "\n",
    "#### 3. Policy improvement\n",
    "1. $policy\\text{-}stable \\leftarrow true$\n",
    "2. For each $s \\in S$:\n",
    "    1. $old\\text{-}action \\leftarrow \\pi(s)$\n",
    "    2. $\\pi(s) \\leftarrow \\argmax_a Q(s, a)$\n",
    "    3. If $old\\text{-}action \\neq \\pi(s)$, then $policy\\text{-}stable \\leftarrow false$\n",
    "3. If $policy\\text{-}stable$, then stop and return $Q \\approx q_*$; else go to Policy Evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 4.6\n",
    "\n",
    "1. Initialization:\n",
    "    * $\\pi$ should satisfy the given condition: $\\pi(s|a) \\geq \\varepsilon/|\\mathcal{A}(s)|$\n",
    "2. Policy evaluation:\n",
    "    * We need to rewrite the Bellman equation to use stochastic policies: $V(s) \\rightarrow \\sum_{a} \\pi(s|a) \\sum_{s', r} p(s', r | s, a) [r + \\gamma V(s)]$\n",
    "3. Policy improvement:\n",
    "    * We need to change the update of $\\pi$:\n",
    "        * $a_{best} \\leftarrow \\argmax_a \\sum_{s', r} p(s', r | s, a) [r + \\gamma V(s)]$\n",
    "        * $\\pi(a|s) \\leftarrow \\varepsilon/|\\mathcal{A}(s)|$ for all $a \\neq a_{best}$\n",
    "        * $\\pi(a_{best}|s) \\leftarrow 1 - \\frac{\\varepsilon}{|\\mathcal{A}(s)|} * (|\\mathcal{A}(s)| - 1)$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
