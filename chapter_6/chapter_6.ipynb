{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.1\n",
    "\n",
    "\n",
    "1. $\\delta_t = R_{t+1} + \\gamma V_t(S_{t+1}) - V_t(S_t)$\n",
    "2. $V_{t+1}(S_t) = V_t(S_t) + \\alpha [R_{t+1} + \\gamma V_t(S_{t+1}) - V_t(S_t)]$\n",
    "3. $V_{k+1}(S_t) = V_k(S_t) + \\alpha [R_{t+2} + \\gamma V_k(S_{t+1}) - V_k(S_t)]$ (generalized 2)\n",
    "\n",
    "$\n",
    "G_t - V_t(S_t) \\\\\n",
    "= R_{t+1} + \\gamma G_{t+1} - V_t(S) \\\\\n",
    "= R_{t+1} + \\gamma V_t(S_{t+1}) - V_t(S_t) + \\gamma G_{t+1} - \\gamma V_t(S_{t+1}) \\\\\n",
    "= \\delta_t + \\gamma G_{t+1} - \\gamma V_t(S_{t+1}) \\\\\n",
    "= \\delta_t + \\gamma [G_{t+1} - V_{t+1}(S_{t+1})] + \\gamma [V_{t+1}(S_{t+1}) - V_t(S_{t+1})]\n",
    "= \\delta_t + \\gamma [G_{t+1} - V_{t+1}(S_{t+1})] + \\gamma [V_t(S_{t+1}) + \\alpha [R_{t+2} + \\gamma V_t(S_{t+2}) - V_t{S_{t+2}}] - V_t(S_{t+1})] \\text{   (from 3.)} \\\\\n",
    "= \\delta_t + \\gamma [G_{t+1} - V_{t+1}(S_{t+1})] + \\alpha \\gamma [R_{t+2} + \\gamma V_t(S_{t+2}) - V_t{S_{t+2}}] \\\\\n",
    "\\text{Now } [G_{t+1} - V_{t+1}(S_{t+1})] \\text{ is the first formula with higher index, so we have recursive unfolding} \\\\\n",
    "= \\delta_t + \\gamma [\\delta_{t+1} + \\gamma[G_{t+2} - V_{t+2}(S_{t+2})] + \\alpha \\gamma [R_{t+3} + \\gamma V_{t+1}(S_{t+3}) - V_{t+1}(S_{t+2})]] + \\alpha \\gamma [R_{t+2} + \\gamma V_t(S_{t+2}) - V_t{S_{t+2}}] \\\\\n",
    "= \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2[G_{t+2} - V_{t+2}(S_{t+2})] + \\alpha [\\gamma^2 [R_{t+3} + \\gamma V_{t+1}(S_{t+3}) - V_{t+1}(S_{t+2})] + \\gamma [R_{t+2} + \\gamma V_t(S_{t+2}) - V_t{S_{t+2}}]] \\\\\n",
    "... \\\\\n",
    "= \\sum_{k=t}^{T-1} \\gamma^{k-t} \\delta_k + \\alpha \\sum_{k=t}^{T-2} \\gamma^{k-t+1} [R_{k+2} + \\gamma V_k(S_{k+2}) - V_k(S_{k+1})]\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.2\n",
    "\n",
    "**Here’s a hint**: Suppose you have lots of experience driving home from work. Then you move to a new building and a new parking lot (but you still enter the highway at the same place). Now you are starting to learn predictions for the new building.\n",
    "\n",
    "Suppose that the old route home is described by trajectory $\\tau_{old} = S_1S_2...S_n$, and the new route by trajectory $tau_{new}=S_1'S_2...S_n$. We have good estimate for $\\tau_{old}$ and for all its states, now we want to estimate $\\tau_{new}$. In case of Monte Carlo, each update of $V(S_1')$ will only use the single episode. In case of TD learning, each update will use the old estimates of $S_2, S_3, ..., S_n$, which are much more accurate than a single episode.\n",
    "\n",
    "The same could happen if we start every time from the same state, but later parts of the episodes are different. For example, There is roadworks and we have to take a detour so $\\tau_{old} = S_1S_2,S_3...S_n$, $\\tau_{new}=S_1S_2',S_3...S_n$. Monte Carlo will estimate $V(S_2')$ based only on episodes done one the new route. TD-learning, will estimate it using acurate estimation of $S_3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.3\n",
    "Since, only the $V(A)$ changed in the first episode, the first episode has to be: $C,0,B,0,A,0$.\n",
    "\n",
    "Then, in the estimation we update $V(C)$, $V(B)$, $V(A)$ in this order. For $V(C)$ we have:\n",
    "$$\n",
    "V(C) \\leftarrow V(C) + \\alpha [R + \\gamma V(B) - V(C)]\n",
    "$$\n",
    "We know that $V(S) = 0.5$ for all $S$, $R = 0$, $\\alpha = 0.1$ and $\\gamma = 1$, so\n",
    "\n",
    "$$\n",
    "V(C) \\leftarrow V(C) + 0.1 \\cdot [0 + 0.5 - 0.5] \n",
    "$$\n",
    "$$\n",
    "V(C) \\leftarrow V(C)\n",
    "$$\n",
    "So $V(C)$ doesnt change, the same for $V(B)$. For $V(A)$ we have\n",
    "$$\n",
    "V(A) \\leftarrow V(A) + \\alpha [R + \\gamma V(terminal) - V(A)]\n",
    "$$\n",
    "$V(terminal) = 0$, so\n",
    "$$\n",
    "V(A) \\leftarrow 0.5 + 0.1 \\cdot [0 + 0 - 0.5]\n",
    "$$\n",
    "$$\n",
    "V(A) \\leftarrow 0.45\n",
    "$$\n",
    "\n",
    "So $V(A)$ changed from 0.5 to 0.45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.4\n",
    "No, the wider range of the step-size parameter $\\alpha$ wouldn't affect the conclusions about which algorithm is better.\n",
    "\n",
    "#### Explanation\n",
    "First, let's look at the error of TD. $\\alpha = 0.05$ achieved the best result - error below $0.05$, error curve becomes flat around episode 75. Lower values of $\\alpha$ would probably reach the same error (or maybe a little smaller) but in more steps.\n",
    "\n",
    "Higher values of $\\alpha$ converged faster during the first 25-50 steps, but later they started to get worse, the error went up. This effect (faster convergence at the beggining and divergence soon) is bigger for $\\alpha = 0.15$ than for $\\alpha = 0.1$ so it would have been even bigger for larger $\\alpha$.\n",
    "\n",
    "Now let's look at MC. For the lowest $\\alpha = 0.01$, the error curve is the most smooth, but the learning is very slow, the error after 100 episodes is significantly higher than for TD (around 0.12). For higher values of $\\alpha$ the algorithm converges faster, after around 50 episodes we can see that the error often goes up in the next steps, more often and higher for larger $\\alpha$, this effect would be greater for larger $\\alpha$ values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6,5*\n",
    "We initialize estimate values $V(S) = 0.5$ for each state $S$, so each estimate except for $V(C)$ is quite far from the true value of the state. At the beginning, with each update, each of these estimates changes toward the true value of the state. The problem starts to occur when the estimates are close to the true values, then the update can move it away from the true value, resulting in higher error.\n",
    "\n",
    "For example, let's assume that estimates for $A$, $B$ $C$ are equal to the true values: $V(A) = \\frac{1}{6}$, $V(B) = \\frac{2}{6}$, $V(C) = \\frac{3}{6}$, and the next episode is $C,0,B,0,A,0$. Then we have the following updates for $V(C), $V(B)$ and $V(A)$\n",
    "\n",
    "* $V(C) \\leftarrow V(C) + \\alpha [0 + V(B) - V(C)] = \\frac{3}{6} + \\alpha [0 + \\frac{2}{6} - \\frac{3}{6}] = \\frac{3}{6} - \\alpha \\frac{1}{6}$\n",
    "* $V(B) \\leftarrow V(B) + \\alpha [0 + V(A) - V(B)] = \\frac{2}{6} + \\alpha [0 + \\frac{1}{6} - \\frac{2}{6}] = \\frac{2}{6} - \\alpha \\frac{1}{6}$\n",
    "* $V(A) \\leftarrow V(A) + \\alpha [0 + V(terminal) - V(A)] = \\frac{2}{6} + \\alpha [0 + 0 - \\frac{1}{6}] = \\frac{1}{6} - \\alpha \\frac{1}{6}$\n",
    "\n",
    "Each estimated value after update is further away from the true value than before update, so the mean square error will be higher after this step. The difference is greater for greater $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.6\n",
    "\n",
    "The true values of states can be found by solving the following system of equations.\n",
    "\n",
    "$V(A) = 0.5 \\cdot 0 + 0.5 \\cdot V(B)\\\\$\n",
    "$V(B) = 0.5 \\cdot V(B) + 0.5 \\cdot V(C)\\\\$\n",
    "$V(C) = 0.5 \\cdot V(B) + 0.5 \\cdot V(D)\\\\$\n",
    "$V(D) = 0.5 \\cdot V(C) + 0.5 \\cdot V(E)\\\\$\n",
    "$V(E) = 0.5 \\cdot V(D) + 0.5 \\cdot 1\\\\$\n",
    "\n",
    "Which can be solved either\n",
    "1. analytically, or\n",
    "2. using dynamic programming.\n",
    "\n",
    "I think that, the analitical solutions was used, since this system is fairly easy to solve. It would probably require less calculations than using dynamic programming.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.7*\n",
    "\n",
    "$$\n",
    "V(S_t) \\leftarrow V(S_t) + \\alpha \\rho_{t:t} [R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.8\n",
    "$\\delta_t = R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)$\n",
    "\n",
    "\\begin{split}\n",
    "G_t - Q(S_t, A_t) &= R_{t+1} + \\gamma G_{t+1} - Q(S_t, A_t) + \\gamma Q(S_{t+1}, A_{t+1}) - \\gamma Q(S_{t+1}, A_{t+1})\\\\\n",
    "                  &= \\delta_t + \\gamma (G_{t+1} - Q(S_{t+1}, A_{t+1}))\\\\\n",
    "                  &= \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2(G_{t+2} - Q(S_{t+2}, A_{t+2}))\\\\\n",
    "                  &= \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2\\gamma \\delta_{t+2} + ... + \\gamma^{T-t-1}\\delta_{T-1} + \\gamma^{T-1}(G_T - Q(S_T, \\cdot))\\\\\n",
    "                  &= \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2\\gamma \\delta_{t+2} + ... + \\gamma^{T-t-1}\\delta_{T-1} + \\gamma^{T-1}(0 - 0)\\\\\n",
    "                  &= \\sum_{k=t}^{T-1}\\gamma^{k-t}\\delta_k\n",
    "\\end{split}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.11\n",
    "\n",
    "Q-learning considered an off-policy control method, because we update different policy than we use to explore the environment:\n",
    "\n",
    "* target policy, which we estimate and update, is the greedy policy derived from Q.\n",
    "* behavior policy is also derived from Q, but it is $\\varepsilon$-greedy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.12\n",
    "\n",
    "If action selection is greedy, then Q-learning is still slightly different than Sarsa.\n",
    "\n",
    "Let's consider an example where in time-step $t$ we are in state $S_t = s$, choose action $A_t$ and end up in the same state $S_{t+1} = s$. Let's also assume that Q update in time $t$ changed the optimal action in state $s$. Q-learning will immediately switch to the new action, Sarsa will again use the old action, because it was chosen before Q update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.13\n",
    "**Update equations for Double Expected Sarsa**\n",
    "\n",
    "We have two estimates of state-action values: $Q_1$, $Q_2$, and 2 $\\varepsilon$-greedy policies: $\\pi_1$ in $Q_1$, $\\pi_2$ in $Q_2$\n",
    "\n",
    "Update rule:\n",
    "\n",
    "$$\n",
    "Q_1(S_t, A_t) \\leftarrow Q_1(S_t, A_t) + \\alpha \\left[R_{t+1} + \\gamma \\mathbb{E}_{\\pi_2}[Q_1(S_{t+1}, A_{t+1}) | S_{t+1}] - Q_1(S_t, A_t) \\right] \\\\\n",
    "= Q_1(S_t, A_t) + \\alpha \\left[R_{t+1} + \\gamma \\sum_{a} \\pi_2(a | S_{t+1})Q_1(S_{t+1}, a) - Q_1(S_t, A_t) \\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "Q_2(S_t, A_t) \\leftarrow Q_2(S_t, A_t) + \\alpha \\left[R_{t+1} + \\gamma \\mathbb{E}_{\\pi_1}[Q_2(S_{t+1}, A_{t+1}) | S_{t+1}] - Q_2(S_t, A_t) \\right] \\\\\n",
    "= Q_2(S_t, A_t) + \\alpha \\left[R_{t+1} + \\gamma \\sum_{a} \\pi_1(a | S_{t+1})Q_2(S_{t+1}, a) - Q_2(S_t, A_t) \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.14\n",
    "**Describe how the task of Jack’s Car Rental (Example 4.2) could be reformulated in terms of afterstates.**\n",
    "\n",
    "Afterstates could represent number of cars in each location in the morning, after moving them during night, but before a first (or return) rent of the next day. This would speed convergence because many of state-action pair would lead to the same afterstate. State is number of cars at the end of a day, action is moving $n$ cars from location 1 to location 2, $n \\in \\{-4, -3, ..., 4\\}$. For example:\n",
    "* State (5, 5) and action 1,\n",
    "* State (4, 6) and action 0,\n",
    "* State (3, 7) and action -1,\n",
    "* and others\n",
    "\n",
    "lead to the same afterstate (4, 6), so they should have the same value. In the original formulation these state-action pairs are independent, their estimates will be independent learn only from part of the data. If we use afterstates (formulate update rule that uses afterstates), all these state-action pairs contribute to the estimation of afterstate (4, 6) and they learn from this estimation, so they learn from each other."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
