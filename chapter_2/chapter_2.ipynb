{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.6\n",
    "\n",
    "1. We start with $Q(A) = 5$ for each $A$\n",
    "2. We randomly choose action $a_1$ and get a reward $R_{a_1}$. $E[R_{a_1}] = a_1.\\mu < 5$\n",
    "3. We update $Q(a_1) = (5 + R_{a_1})/2 \\approx (5 + a_1.\\mu)/2 < 5$\n",
    "4. We randomly choose $a_2$ from remaining actions. $a_2 \\neq a1$. We update $Q(a_2)$ and so on.\n",
    "5. After 10 steps $Q(A) = (5 + A.R) / 2 \\approx (5 + A.\\mu)$, so there is a higher chance to choose optimal action in step 11.\n",
    "6. After step 11 $Q(a_{11})$ goes down. If $a_{11}$ is optimal then we wan't choose it in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.8\n",
    "Similar to 2.6\n",
    "1. In first 10 steps we randomly pick one of each action\n",
    "2. In step 11, exploration term is equal for all actions, so $Q_t(a)$ decides which action to choose, so we pick the action which is probably the best, so the average reward goes up.\n",
    "3. In step 12, the optimal action from step 11 (let's call it $a_{opt}$) won't be chosen because $N_t(a_{opt}) = 2$ and $N_t(a) = 1$ for $a \\neq a_{opt}$, so the average reward goes down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.10\n",
    "Reward for $A_1$: $P(R = 10 | A1) = 0.5$, $P(R = 90 | A1) = 0.5$\n",
    "Reward for $A_2$: $P(R = 20 | A1) = 0.5$, $P(R = 80 | A1) = 0.5$\n",
    "\n",
    "Suppose we choose $A_1$ with probability $p$ and $A_2$ with probability $1-p$, then\n",
    "$$\n",
    "E[R] = p * (0.5*10 + 0.5*90) + (1-p) * (0.5*20 + 0.5*80) = p*50 + (1-p)*50 = 50p + 50 - 50p = 50\n",
    "$$\n",
    "so the expected reward doesn't depend of how we choose actions."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
