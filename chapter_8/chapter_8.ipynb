{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f8bae42",
   "metadata": {},
   "source": [
    "### Exercise 8.1\n",
    "It is unlikely that multi-step bootstrapping methods will perform as well as the Dyna method.Yes, first episode will update $k$ state-action values instead of one, but in the second episode it will still take quite many steps to reach one of that updated states and get the next updates.\n",
    "\n",
    "With the Dyna method, during the second episode, in each step $n=50$ times we randomly choose a state-action pair from a previous episode. If the last one or the penultimate one is chosen, it will be updated because $R>0$ or $\\max_a Q(S', a) > 0$. If the penultimate state-action pair is updated then the third from the last state-action can be updated when it is randomly chosen etc. So there is a quite high chance of multiple updates during the Dyna method, but the chances are low for non-planning method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319584c0",
   "metadata": {},
   "source": [
    "### Exercise 8.2\n",
    "Dyna-Q+ performed better in the first phase because it explored more and found more efficient route. There is a high chance that the initial agent will first find not efficient paths, in which it is easy to do the \"shortcut\". Dyna-Q+ explores more and has more possibilities for finding these shortcuts.\n",
    "\n",
    "Similar for the second phase, Dyna-Q+ performed more exploration and was able to find quicker (or at all) the new optimal path."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f1d846",
   "metadata": {},
   "source": [
    "### Exercise 8.3\n",
    "\n",
    "Dyna-Q+ explores more during the whole experiment. The result of more exploration is that it quicker found optimal path than Dyna-Q. But when both algorithms already found optimal path, Dyna-Q+ still sometimes explored, but Dyna-Q didn't (it always followed the optimal path). So Dyna-Q had higher rewards than Dyna-Q+, during the episode between finding the optimal path and the enviroment change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9232426c",
   "metadata": {},
   "source": [
    "### Exercise 8.5\n",
    "For stochastic environment the should remember how many times after choosing action $A$ in state $S$, agent ended up in state $S'$ and reward $R$, which gives us the estimated average probability. $Model(S, A)$ should return $R, S'$ based on this estimated probability. This will work poorly for changing environment beacuse past transitions before enviroment change (which are no longer true after change) will be as important as transitions after change, so the model will not fully learn the new probabilities. The estimated probabilities will be some average between false and true probabilities. The algorithm can be modified to use some kind of running average instead of simple average. This will make new experience more important than the old one, in time the old one will almost completely disappear, so the model will come much closer to the real probabilities."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
